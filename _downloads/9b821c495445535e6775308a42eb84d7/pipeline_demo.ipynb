{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Ab-initio Pipeline Demonstration\n\nThis tutorial demonstrates some key components of an ab-initio\nreconstruction pipeline using synthetic data generated with ASPIRE's\n``Simulation`` class of objects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download an Example Volume\nWe begin by downloading a high resolution volume map of the 80S\nRibosome, sourced from EMDB: https://www.ebi.ac.uk/emdb/EMD-2660.\nThis is one of several volume maps that can be downloaded with\nASPIRE's data downloading utility by using the following import.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.downloader import emdb_2660\n\n# Load 80s Ribosome as a ``Volume`` object.\noriginal_vol = emdb_2660()\n\n# During the preprocessing stages of the pipeline we will downsample\n# the images to an image size of 64 pixels. Here, we also downsample the\n# volume so we can compare to our reconstruction later.\nres = 64\nvol_ds = original_vol.downsample(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>A ``Volume`` can be saved using the ``Volume.save()`` method as follows::\n\n        fn = f\"downsampled_80s_ribosome_size{res}.mrc\"\n        vol_ds.save(fn, overwrite=True)</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Simulation Source\nASPIRE's ``Simulation`` class can be used to generate a synthetic\ndataset of projection images.  A ``Simulation`` object produces\nrandom projections of a supplied Volume and applies noise and CTF\nfilters. The resulting stack of 2D images is stored in an ``Image``\nobject.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CTF Filters\nLet's start by creating CTF filters. The ``operators`` package\ncontains a collection of filter classes that can be supplied to a\n``Simulation``.  We use ``RadialCTFFilter`` to generate a set of CTF\nfilters with various defocus values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create CTF filters\nimport numpy as np\n\nfrom aspire.operators import RadialCTFFilter\n\n# Radial CTF Filter\ndefocus_min = 15000  # unit is angstroms\ndefocus_max = 25000\ndefocus_ct = 7\n\nctf_filters = [\n    RadialCTFFilter(defocus=d)\n    for d in np.linspace(defocus_min, defocus_max, defocus_ct)\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Simulation Object\nWe feed our ``Volume`` and filters into ``Simulation`` to generate\nthe dataset of images.  When controlled white Gaussian noise is\ndesired, ``WhiteNoiseAdder(var=VAR)`` can be used to generate a\nsimulation data set around a specific noise variance.\n\nAlternatively, users can bring their own images using an\n``ArrayImageSource``, or define their own custom noise functions via\n``Simulation(..., noise_adder=CustomNoiseAdder(...))``.  Examples\ncan be found in ``tutorials/class_averaging.py`` and\n``experiments/simulated_abinitio_pipeline.py``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.noise import WhiteNoiseAdder\nfrom aspire.source import Simulation\n\n# For this ``Simulation`` we set all 2D offset vectors to zero,\n# but by default offset vectors will be randomly distributed.\n# We cache the Simulation to prevent regenerating the projections\n# for each preprocessing stage.\nsrc = Simulation(\n    n=2500,  # number of projections\n    vols=original_vol,  # volume source\n    offsets=0,  # Default: images are randomly shifted\n    unique_filters=ctf_filters,\n    noise_adder=WhiteNoiseAdder(var=0.0002),  # desired noise variance\n).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The noise variance value above was chosen based on other parameters for this\n  quick tutorial, and can be changed to adjust the power of the additive noise.\n  Alternatively, an SNR value can be prescribed as follows::\n\n      Simulation(..., noise_adder=WhiteNoiseAdder.from_snr(SNR))</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Several Views of the Projection Images\nWe can access several views of the projection images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# with no corruption applied\nsrc.projections[0:10].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# with no noise corruption\nsrc.clean_images[0:10].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# with noise and CTF corruption\nsrc.images[0:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Preprocessing\nWe apply some image preprocessing techniques to prepare the\nthe images for denoising via Class Averaging.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downsampling\nWe downsample the images. Reducing the image size will improve the\nefficiency of subsequent pipeline stages. Metadata such as pixel size is\nscaled accordingly to correspond correctly with the image resolution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src = src.downsample(res)\nsrc.images[:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CTF Correction\nWe apply ``phase_flip()`` to correct for CTF effects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src = src.phase_flip()\nsrc.images[:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize Background\nWe apply ``normalize_background()`` to prepare the image class averaging.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src = src.normalize_background()\nsrc.images[:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Noise Whitening\nWe apply ``whiten()`` to estimate and whiten the noise.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src = src.whiten()\nsrc.images[:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contrast Inversion\nWe apply ``invert_contrast()`` to ensure a positive valued signal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src = src.invert_contrast()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caching\nWe apply ``cache`` to store the results of the ``ImageSource``\npipeline up to this point.  This is optional, but can provide\nbenefit when used intently on machines with adequate memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src = src.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Averaging\nFor this tutorial we use the ``DebugClassAvgSource`` to generate an ``ImageSource``\nof class averages. Internally, ``DebugClassAvgSource`` uses the ``RIRClass2D``\nobject to classify the source images via the rotationally invariant representation\n(RIR) algorithm and the ``TopClassSelector`` object to select the first ``n_classes``\nimages in the original order from the source. In practice, class selection is commonly\ndone by sorting class averages based on some configurable notion of quality\n(contrast, neighbor distance etc) which can be accomplished by providing a custom\nclass selector to ``ClassAverageSource``, which changes the ordering of the classes\nreturned by ``ClassAverageSource``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.denoising import DebugClassAvgSource\n\navgs = DebugClassAvgSource(src=src)\n\n# We'll continue our pipeline using only the first ``n_classes`` from\n# ``avgs``.  The ``cache()`` call is used here to precompute results\n# for the ``:n_classes`` slice.  This avoids recomputing the same\n# images twice when peeking in the next cell then requesting them in\n# the following ``CLSyncVoting`` algorithm.  Outside of demonstration\n# purposes, where we are repeatedly peeking at various stage results,\n# such caching can be dropped allowing for more lazy evaluation.\nn_classes = 250\navgs = avgs[:n_classes].cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View the Class Averages\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Show class averages\navgs.images[0:10].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Show original images corresponding to those classes. This 1:1\n# comparison is only expected to work because we used\n# ``TopClassSelector`` to classify our images.\nsrc.images[0:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Orientation Estimation\nWe create an ``OrientedSource``, which consumes an ``ImageSource`` object, an\norientation estimator, and returns a new source which lazily estimates orientations.\nIn this case we supply ``avgs`` for our source and a ``CLSyncVoting``\nclass instance for our orientation estimator. The ``CLSyncVoting`` algorithm employs\na common-lines method with synchronization and voting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.abinitio import CLSyncVoting\nfrom aspire.source import OrientedSource\nfrom aspire.utils import Rotation\n\n# Stash true rotations for later comparison\ntrue_rotations = Rotation(src.rotations[:n_classes])\n\n# Instantiate a ``CLSyncVoting`` object for estimating orientations.\norient_est = CLSyncVoting(avgs)\n\n# Instantiate an ``OrientedSource``.\noriented_src = OrientedSource(avgs, orient_est)\n\n# Estimate Rotations.\nest_rotations = oriented_src.rotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Error of Estimated Rotations\nASPIRE has the built-in utility function, ``mean_aligned_angular_distance``, which globally\naligns the estimated rotations to the true rotations and computes the mean\nangular distance (in degrees).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.utils import mean_aligned_angular_distance\n\n# Compare with known true rotations\nmean_ang_dist = mean_aligned_angular_distance(est_rotations, true_rotations)\nprint(f\"Mean aligned angular distance: {mean_ang_dist} degrees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Volume Reconstruction\nNow that we have our class averages and rotation estimates, we can\nestimate the mean volume by supplying the class averages and basis\nfor back projection.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.reconstruction import MeanEstimator\n\n# Setup an estimator to perform the back projection.\nestimator = MeanEstimator(oriented_src)\n\n# Perform the estimation and save the volume.\nestimated_volume = estimator.estimate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of Estimated Volume with Source Volume\nTo get a visual confirmation that our results are sane, we rotate\nthe estimated volume by the estimated rotations and project along\nthe z-axis.  These estimated projections should align with the\noriginal projection images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the first 10 projections from the estimated volume using the\n# estimated orientations.  Recall that ``project`` returns an\n# ``Image`` instance, which we can peek at with ``show``.\nprojections_est = estimated_volume.project(est_rotations[0:10])\n\n# We view the first 10 projections of the estimated volume.\nprojections_est.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For comparison, we view the first 10 source projections.\nsrc.projections[0:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fourier Shell Correlation\nAdditionally, we can compare our reconstruction to the known source volume\nby performing a Fourier shell correlation (FSC). We first find a rotation\nmatrix which best aligns the estimated rotations to the ground truth rotations\nusing the ``find_registration`` method. We then use that rotation to align\nthe reconstructed volume to the ground truth volume.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# `find_registration` returns the best aligning rotation, `Q`, as well as\n# a `flag` which indicates if the volume needs to be reflected.\nQ, flag = Rotation(est_rotations).find_registration(true_rotations)\naligned_vol = estimated_volume\nif flag == 1:\n    aligned_vol = aligned_vol.flip()\naligned_vol = aligned_vol.rotate(Rotation(Q.T))\n\n# Compute the FSC.\nvol_ds.fsc(aligned_vol, cutoff=0.143, plot=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}